"""Deep Research Agent implementation."""

from langchain_aws import ChatBedrock
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import END, StateGraph

from ..config import Settings
from ..tools import SearchTools
from .state import ResearchState


class DeepResearchAgent:
    """Deep Research Agent using LangGraph."""

    def __init__(self, settings: Settings) -> None:
        """Initialize agent."""
        self.settings = settings
        self.llm = ChatBedrock(
            model_id=settings.model_id,
            region_name=settings.aws_region,
            model_kwargs={"temperature": settings.temperature, "max_tokens": settings.max_tokens},
        )
        self.search_tools = SearchTools()
        self.graph = self._build_graph()

    def _generate_subqueries(self, state: ResearchState) -> ResearchState:
        """Generate subqueries from main query."""
        print("\nðŸ¤” ã‚¹ãƒ†ãƒƒãƒ— 1/4: ã‚µãƒ–ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆä¸­...")
        prompt = f"""Research Query: "{state['query']}"

ã“ã®è³ªå•ã«åŒ…æ‹¬çš„ã«ç­”ãˆã‚‹ãŸã‚ã®ã€3-5å€‹ã®å…·ä½“çš„ãªSub Queryã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
Sub Queryã®ã¿ã‚’1è¡Œãšã¤è¿”ã—ã¦ãã ã•ã„ã€‚"""

        response = self.llm.invoke([SystemMessage(content="ã‚ãªãŸã¯Research Assistantã§ã™ã€‚"), HumanMessage(content=prompt)])
        subqueries = [q.strip() for q in response.content.split("\n") if q.strip()]
        print(f"âœ“ {len(subqueries)}å€‹ã®ã‚µãƒ–ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        return {"subqueries": subqueries}

    def _search_sources(self, state: ResearchState) -> ResearchState:
        """Search multiple sources for information."""
        print("\nðŸ” ã‚¹ãƒ†ãƒƒãƒ— 2/4: è¤‡æ•°ã‚½ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’æ¤œç´¢ä¸­...")
        results = []
        for i, subquery in enumerate(state["subqueries"], 1):
            print(f"  [{i}/{len(state['subqueries'])}] {subquery}")
            arxiv_results = self.search_tools.search_arxiv(subquery, max_results=3)
            web_results = self.search_tools.search_web(subquery, max_results=3)
            kaggle_comps = self.search_tools.search_kaggle_competitions(subquery)
            kaggle_datasets = self.search_tools.search_kaggle_datasets(subquery)
            kaggle_notebooks = self.search_tools.search_kaggle_notebooks(subquery)
            kaggle_discussions = self.search_tools.search_kaggle_discussions(subquery)

            results.extend([{"query": subquery, "source": "arxiv", **r} for r in arxiv_results])
            results.extend([{"query": subquery, "source": "web", **r} for r in web_results])
            results.extend([{"query": subquery, "source": "kaggle-competition", **r} for r in kaggle_comps])
            results.extend([{"query": subquery, "source": "kaggle-dataset", **r} for r in kaggle_datasets])
            results.extend([{"query": subquery, "source": "kaggle-notebook", **r} for r in kaggle_notebooks])
            results.extend([{"query": subquery, "source": "kaggle-discussion", **r} for r in kaggle_discussions])
        print(f"âœ“ {len(results)}å€‹ã®ã‚½ãƒ¼ã‚¹ã‚’åŽé›†ã—ã¾ã—ãŸ")
        return {"search_results": results}

    def _generate_outline(self, state: ResearchState) -> ResearchState:
        """Generate article outline from search results."""
        print("\nðŸ“‹ ã‚¹ãƒ†ãƒƒãƒ— 3/4: è¨˜äº‹ã®ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã‚’ç”Ÿæˆä¸­...")
        results_text = "\n\n".join([f"- {r['title']}: {r.get('summary', r.get('content', ''))[:200]}" for r in state["search_results"]])
        prompt = f"""ã‚¯ã‚¨ãƒª: "{state['query']}"

æ¤œç´¢çµæžœ:
{results_text}

ã“ã‚Œã‚‰ã®æƒ…å ±ã«åŸºã¥ã„ã¦ã€è©³ç´°ãªè¨˜äº‹ã®ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚
æ—¥æœ¬èªžã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

        response = self.llm.invoke([SystemMessage(content="ã‚ãªãŸã¯Research Writerã§ã™ã€‚"), HumanMessage(content=prompt)])
        print("âœ“ ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        return {"outline": response.content}

    def _generate_article(self, state: ResearchState) -> ResearchState:
        """Generate final article."""
        print("\nðŸ“ ã‚¹ãƒ†ãƒƒãƒ— 4/4: æœ€çµ‚è¨˜äº‹ã‚’ç”Ÿæˆä¸­...")
        results_text = "\n\n".join([f"[{r['source']}] {r['title']}\n{r.get('summary', r.get('content', ''))}\nURL: {r['url']}" for r in state["search_results"]])
        prompt = f"""ä»¥ä¸‹ã®ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã«å¾“ã£ã¦ã€åŒ…æ‹¬çš„ãªãƒªã‚µãƒ¼ãƒè¨˜äº‹ã‚’æ—¥æœ¬èªžã§åŸ·ç­†ã—ã¦ãã ã•ã„:

{state['outline']}

ä»¥ä¸‹ã®æƒ…å ±æºã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„:
{results_text}

å„ä¸»å¼µã®å¾Œã«å¼•ç”¨URL [source] ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
å…¨ã¦æ—¥æœ¬èªžã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

        response = self.llm.invoke([SystemMessage(content="ã‚ãªãŸã¯Research Writerã§ã™ã€‚"), HumanMessage(content=prompt)])
        print("âœ“ è¨˜äº‹ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        return {"article": response.content}

    def _build_graph(self) -> StateGraph:
        """Build the research workflow graph."""
        workflow = StateGraph(ResearchState)
        workflow.add_node("generate_subqueries", self._generate_subqueries)
        workflow.add_node("search_sources", self._search_sources)
        workflow.add_node("generate_outline", self._generate_outline)
        workflow.add_node("generate_article", self._generate_article)

        workflow.set_entry_point("generate_subqueries")
        workflow.add_edge("generate_subqueries", "search_sources")
        workflow.add_edge("search_sources", "generate_outline")
        workflow.add_edge("generate_outline", "generate_article")
        workflow.add_edge("generate_article", END)

        return workflow.compile()

    def research(self, query: str) -> dict:
        """Run research workflow."""
        initial_state = {"query": query, "subqueries": [], "outline": "", "search_results": [], "article": "", "messages": []}
        result = self.graph.invoke(initial_state)
        return result
