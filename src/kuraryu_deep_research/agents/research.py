"""Deep Research Agent implementation."""

import boto3
from botocore.config import Config
from langchain_aws import ChatBedrock
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import END, StateGraph

from ..config import Settings
from ..tools import SearchTools
from .state import ResearchState

MAX_ITERATIONS = 3
MAX_DEPTH = 2


class DeepResearchAgent:
    """Deep Research Agent using LangGraph."""

    def __init__(self, settings: Settings) -> None:
        """Initialize agent."""
        self.settings = settings
        boto_config = Config(read_timeout=6000, retries={"max_attempts": 3})
        bedrock_client = boto3.client(
            "bedrock-runtime",
            region_name=settings.aws_region,
            config=boto_config,
        )
        self.llm = ChatBedrock(
            model_id=settings.model_id,
            client=bedrock_client,
            model_kwargs={"temperature": settings.temperature, "max_tokens": settings.max_tokens},
        )
        self.search_tools = SearchTools()
        self.graph = self._build_graph()

    def _generate_subqueries(self, state: ResearchState) -> ResearchState:
        """Generate subqueries from main query or gaps."""
        iteration = state.get("iteration", 0)
        
        if iteration == 0:
            print("\nü§î „Çπ„ÉÜ„ÉÉ„Éó 1: „Çµ„Éñ„ÇØ„Ç®„É™„ÇíÁîüÊàê‰∏≠...")
            target = state["query"]
        else:
            print(f"\nüîÑ ÂèçÂæ© {iteration + 1}: ‰∏çË∂≥ÊÉÖÂ†±„ÇíË£úÂÆå„Åô„Çã„ÇØ„Ç®„É™„ÇíÁîüÊàê‰∏≠...")
            target = "‰∏çË∂≥„Åó„Å¶„ÅÑ„ÇãË¶≥ÁÇπ:\n" + "\n".join(state.get("gaps", []))

        prompt = f"""Research Query: "{state['query']}"

{target}

„Åì„ÅÆË≥™Âïè„Å´ÂåÖÊã¨ÁöÑ„Å´Á≠î„Åà„Çã„Åü„ÇÅ„ÅÆ„ÄÅ3-5ÂÄã„ÅÆÂÖ∑‰ΩìÁöÑ„Å™Sub Query„ÇíÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
Sub Query„ÅÆ„Åø„Çí1Ë°å„Åö„Å§Ëøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

        response = self.llm.invoke([SystemMessage(content="„ÅÇ„Å™„Åü„ÅØResearch Assistant„Åß„Åô„ÄÇ"), HumanMessage(content=prompt)])
        subqueries = [q.strip() for q in response.content.split("\n") if q.strip() and not q.startswith("#")]
        print(f"‚úì {len(subqueries)}ÂÄã„ÅÆ„Çµ„Éñ„ÇØ„Ç®„É™„ÇíÁîüÊàê„Åó„Åæ„Åó„Åü")
        
        existing = state.get("subqueries", [])
        return {"subqueries": existing + subqueries, "iteration": iteration + 1}

    def _search_sources(self, state: ResearchState) -> ResearchState:
        """Search multiple sources for information."""
        iteration = state.get("iteration", 1)
        print(f"\nüîç Ê§úÁ¥¢‰∏≠ (ÂèçÂæ© {iteration}/{MAX_ITERATIONS})...")

        results = list(state.get("search_results", []))
        new_queries = state["subqueries"][-5:]
        low_result_queries = []

        for i, subquery in enumerate(new_queries, 1):
            print(f"  [{i}/{len(new_queries)}] {subquery}")
            arxiv_results = self.search_tools.search_arxiv(subquery, max_results=3)
            web_results = self.search_tools.search_web(subquery, max_results=3)
            kaggle_comps = self.search_tools.search_kaggle_competitions(subquery)
            kaggle_datasets = self.search_tools.search_kaggle_datasets(subquery)

            query_results = []
            query_results.extend([{"query": subquery, "source": "arxiv", **r} for r in arxiv_results])
            query_results.extend([{"query": subquery, "source": "web", **r} for r in web_results])
            query_results.extend([{"query": subquery, "source": "kaggle-competition", **r} for r in kaggle_comps])
            query_results.extend([{"query": subquery, "source": "kaggle-dataset", **r} for r in kaggle_datasets])

            if len(query_results) < 2:
                low_result_queries.append(subquery)
            results.extend(query_results)

        # ÁµêÊûú„ÅåÂ∞ë„Å™„ÅÑ„ÇØ„Ç®„É™„ÇíÊîπÂñÑ„Åó„Å¶ÂÜçÊ§úÁ¥¢
        if low_result_queries:
            print(f"\nüîß {len(low_result_queries)}ÂÄã„ÅÆ„ÇØ„Ç®„É™„ÇíÊîπÂñÑ‰∏≠...")
            improved = self._improve_queries(low_result_queries, state["query"])
            for subquery in improved:
                print(f"  ‚Üí {subquery}")
                arxiv_results = self.search_tools.search_arxiv(subquery, max_results=3)
                web_results = self.search_tools.search_web(subquery, max_results=3)
                results.extend([{"query": subquery, "source": "arxiv", **r} for r in arxiv_results])
                results.extend([{"query": subquery, "source": "web", **r} for r in web_results])

        print(f"‚úì ÂêàË®à {len(results)}ÂÄã„ÅÆ„ÇΩ„Éº„Çπ„ÇíÂèéÈõÜ")
        return {"search_results": results}

    def _improve_queries(self, queries: list[str], original_query: str) -> list[str]:
        """Improve queries that returned few results."""
        prompt = f"""ÂÖÉ„ÅÆ„ÇØ„Ç®„É™: "{original_query}"

‰ª•‰∏ã„ÅÆÊ§úÁ¥¢„ÇØ„Ç®„É™„ÅØÁµêÊûú„ÅåÂ∞ë„Å™„Åã„Å£„Åü„Åß„Åô:
{chr(10).join(f'- {q}' for q in queries)}

ÂêÑ„ÇØ„Ç®„É™„ÇíË®Ä„ÅÑÊèõ„Åà„Å¶„ÄÅ„Çà„ÇäÊ§úÁ¥¢ÁµêÊûú„ÅåÂæó„Çâ„Çå„ÇÑ„Åô„ÅÑÂΩ¢„Å´ÊîπÂñÑ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
- Â∞ÇÈñÄÁî®Ë™û„Çí‰∏ÄËà¨ÁöÑ„Å™Ë®ÄËëâ„Å´
- Ëã±Ë™û„ÅÆ„Ç≠„Éº„ÉØ„Éº„Éâ„ÇíËøΩÂä†
- „Çà„ÇäÂ∫É„ÅÑÊ¶ÇÂøµ„Å´Â§âÊõ¥

ÊîπÂñÑ„Åó„Åü„ÇØ„Ç®„É™„ÅÆ„Åø„Çí1Ë°å„Åö„Å§Âá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

        response = self.llm.invoke([
            SystemMessage(content="„ÅÇ„Å™„Åü„ÅØÊ§úÁ¥¢„ÇØ„Ç®„É™ÊúÄÈÅ©Âåñ„ÅÆÂ∞ÇÈñÄÂÆ∂„Åß„Åô„ÄÇ"),
            HumanMessage(content=prompt)
        ])
        return [q.strip().lstrip("-‚Ä¢").strip() for q in response.content.split("\n") if q.strip()][:len(queries)]

    def _evaluate_coverage(self, state: ResearchState) -> ResearchState:
        """Evaluate if collected information is sufficient."""
        print("\nüìä ÊÉÖÂ†±„ÅÆÁ∂≤ÁæÖÊÄß„ÇíË©ï‰æ°‰∏≠...")
        
        results_summary = "\n".join([f"- [{r['source']}] {r['title']}" for r in state["search_results"][:30]])
        
        prompt = f"""„ÇØ„Ç®„É™: "{state['query']}"

ÂèéÈõÜ„Åó„ÅüÊÉÖÂ†±Ê∫ê:
{results_summary}

„Åì„ÅÆÊÉÖÂ†±„ÅßÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Å´ÂçÅÂàÜÁ≠î„Åà„Çâ„Çå„Åæ„Åô„ÅãÔºü
- ÂçÅÂàÜ„Å™Â†¥Âêà: „ÄåSUFFICIENT„Äç„Å®„Å†„ÅëÂõûÁ≠î
- ‰∏çË∂≥„Åå„ÅÇ„ÇãÂ†¥Âêà: ‰∏çË∂≥„Åó„Å¶„ÅÑ„ÇãË¶≥ÁÇπ„ÇíÁÆáÊù°Êõ∏„Åç„ÅßÂàóÊåôÔºàÊúÄÂ§ß3„Å§Ôºâ"""

        response = self.llm.invoke([SystemMessage(content="„ÅÇ„Å™„Åü„ÅØResearchË©ï‰æ°ËÄÖ„Åß„Åô„ÄÇ"), HumanMessage(content=prompt)])
        
        if "SUFFICIENT" in response.content:
            print("‚úì ÊÉÖÂ†±„ÅØÂçÅÂàÜ„Åß„Åô")
            return {"needs_more_search": False, "gaps": []}
        
        gaps = [line.strip().lstrip("-‚Ä¢").strip() for line in response.content.split("\n") if line.strip() and not line.startswith("‰∏çË∂≥")]
        gaps = [g for g in gaps if g][:3]
        print(f"‚ö† ‰∏çË∂≥„Åó„Å¶„ÅÑ„ÇãË¶≥ÁÇπ: {len(gaps)}ÂÄã")
        for gap in gaps:
            print(f"  - {gap}")
        return {"needs_more_search": True, "gaps": gaps}

    def _should_continue_search(self, state: ResearchState) -> str:
        """Decide whether to continue searching or proceed to deep dive."""
        if state.get("needs_more_search") and state.get("iteration", 0) < MAX_ITERATIONS:
            return "generate_subqueries"
        return "deep_dive"

    def _deep_dive(self, state: ResearchState) -> ResearchState:
        """Extract key references from results and explore them deeper."""
        depth = state.get("depth", 0)
        if depth >= MAX_DEPTH:
            return {}

        print(f"\nüî¨ Ê∑±Êéò„ÇäË™øÊüª‰∏≠ (Ê∑±Â∫¶ {depth + 1}/{MAX_DEPTH})...")

        explored = state.get("explored_urls") or set()
        arxiv_results = [r for r in state["search_results"] if r["source"] == "arxiv" and r["url"] not in explored]

        if not arxiv_results:
            print("  Ê∑±Êéò„ÇäÂØæË±°„Å™„Åó")
            return {"depth": depth + 1}

        # ÈáçË¶Å„Å™Ë´ñÊñá„ÇíÁâπÂÆö
        prompt = f"""‰ª•‰∏ã„ÅÆË´ñÊñá„Åã„Çâ„ÄÅÂÖÉ„ÅÆ„ÇØ„Ç®„É™„Äå{state['query']}„Äç„ÇíÊ∑±„ÅèÁêÜËß£„Åô„Çã„Åü„ÇÅ„Å´
„Åï„Çâ„Å´Ë™øÊüª„Åô„Åπ„ÅçÊúÄ„ÇÇÈáçË¶Å„Å™Ë´ñÊñá„ÇíÊúÄÂ§ß2„Å§ÈÅ∏„Çì„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ

Ë´ñÊñá„É™„Çπ„Éà:
{chr(10).join(f"- {r['title']}" for r in arxiv_results[:10])}

ÈÅ∏„Çì„Å†Ë´ñÊñá„ÅÆ„Çø„Ç§„Éà„É´„ÅÆ„Åø„Çí1Ë°å„Åö„Å§Âá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

        response = self.llm.invoke([
            SystemMessage(content="„ÅÇ„Å™„Åü„ÅØResearch Assistant„Åß„Åô„ÄÇ"),
            HumanMessage(content=prompt)
        ])

        selected_titles = [t.strip().lstrip("-‚Ä¢").strip() for t in response.content.split("\n") if t.strip()]
        selected = [r for r in arxiv_results if any(t in r["title"] for t in selected_titles)][:2]

        if not selected:
            return {"depth": depth + 1}

        # ÈÅ∏„Çì„Å†Ë´ñÊñá„ÅÆÈñ¢ÈÄ£Á†îÁ©∂„ÇíÊ§úÁ¥¢
        new_results = []
        new_explored = set(explored)
        for paper in selected:
            print(f"  ‚Üí {paper['title'][:50]}...")
            new_explored.add(paper["url"])

            # Ë´ñÊñá„Çø„Ç§„Éà„É´„ÅßÈñ¢ÈÄ£Á†îÁ©∂„ÇíÊ§úÁ¥¢
            related = self.search_tools.search_arxiv(paper["title"], max_results=3)
            for r in related:
                if r["url"] not in new_explored:
                    new_results.append({"query": f"related to: {paper['title']}", "source": "arxiv-deep", **r})
                    new_explored.add(r["url"])

        print(f"  ‚úì {len(new_results)}ÂÄã„ÅÆÈñ¢ÈÄ£Ë´ñÊñá„ÇíÁô∫Ë¶ã")
        return {
            "search_results": state["search_results"] + new_results,
            "depth": depth + 1,
            "explored_urls": new_explored,
        }

    def _should_continue_deep_dive(self, state: ResearchState) -> str:
        """Decide whether to continue deep diving."""
        if state.get("depth", 0) < MAX_DEPTH:
            new_deep_results = [r for r in state["search_results"] if r["source"] == "arxiv-deep"]
            if new_deep_results:
                return "deep_dive"
        return "verify_information"

    def _verify_information(self, state: ResearchState) -> ResearchState:
        """Verify information across sources and detect contradictions."""
        print("\nüîç ÊÉÖÂ†±„ÅÆÊ§úË®º„Éª„ÇØ„É≠„Çπ„ÉÅ„Çß„ÉÉ„ÇØ‰∏≠...")

        results_text = "\n\n".join(
            [f"[{i+1}] [{r['source']}] {r['title']}\n{r.get('summary', r.get('content', ''))[:300]}"
             for i, r in enumerate(state["search_results"][:20])]
        )

        prompt = f"""„ÇØ„Ç®„É™: "{state['query']}"

ÂèéÈõÜ„Åó„ÅüÊÉÖÂ†±:
{results_text}

‰ª•‰∏ã„ÅÆË¶≥ÁÇπ„ÅßÊÉÖÂ†±„ÇíÊ§úË®º„Åó„Å¶„Åè„Å†„Åï„ÅÑ:
1. ÁüõÁõæ„Åô„Çã‰∏ªÂºµ: Áï∞„Å™„Çã„ÇΩ„Éº„ÇπÈñì„ÅßÁüõÁõæ„Åô„ÇãÊÉÖÂ†±„Åå„ÅÇ„Çå„Å∞ÊåáÊëò
2. ‰ø°È†ºÊÄßË©ï‰æ°: Â≠¶Ë°ìË´ñÊñá(arxiv)„ÅØÈ´ò‰ø°È†º„ÄÅ‰∏ÄËà¨WebË®ò‰∫ã„ÅØË¶ÅÊ≥®ÊÑè
3. ÊÉÖÂ†±„ÅÆÈÆÆÂ∫¶: Âè§„ÅÑÊÉÖÂ†±„Å®Êñ∞„Åó„ÅÑÊÉÖÂ†±„ÅÆÈÅï„ÅÑ„Åå„ÅÇ„Çå„Å∞ÊåáÊëò

Ê§úË®º„É¨„Éù„Éº„Éà„ÇíÁ∞°ÊΩî„Å´Êó•Êú¨Ë™û„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÁüõÁõæ„Åå„Å™„Åë„Çå„Å∞„Äå‰∏ªË¶Å„Å™ÁüõÁõæ„ÅØÊ§úÂá∫„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„Åü„Äç„Å®Ë®òËºâ„ÄÇ"""

        response = self.llm.invoke([
            SystemMessage(content="„ÅÇ„Å™„Åü„ÅØÊÉÖÂ†±Ê§úË®º„ÅÆÂ∞ÇÈñÄÂÆ∂„Åß„Åô„ÄÇ"),
            HumanMessage(content=prompt)
        ])

        print("‚úì Ê§úË®ºÂÆå‰∫Ü")
        return {"verification_report": response.content}

    def _generate_outline(self, state: ResearchState) -> ResearchState:
        """Generate article outline from search results."""
        print("\nüìã Ë®ò‰∫ã„ÅÆ„Ç¢„Ç¶„Éà„É©„Ç§„É≥„ÇíÁîüÊàê‰∏≠...")
        results_text = "\n\n".join([f"- {r['title']}: {r.get('summary', r.get('content', ''))[:200]}" for r in state["search_results"]])
        prompt = f"""„ÇØ„Ç®„É™: "{state['query']}"

Ê§úÁ¥¢ÁµêÊûú:
{results_text}

„Åì„Çå„Çâ„ÅÆÊÉÖÂ†±„Å´Âü∫„Å•„ÅÑ„Å¶„ÄÅË©≥Á¥∞„Å™Ë®ò‰∫ã„ÅÆ„Ç¢„Ç¶„Éà„É©„Ç§„É≥„Çí„Çª„ÇØ„Ç∑„Éß„É≥„Å®„Çµ„Éñ„Çª„ÇØ„Ç∑„Éß„É≥„Åß‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
Êó•Êú¨Ë™û„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

        response = self.llm.invoke([SystemMessage(content="„ÅÇ„Å™„Åü„ÅØResearch Writer„Åß„Åô„ÄÇ"), HumanMessage(content=prompt)])
        print("‚úì „Ç¢„Ç¶„Éà„É©„Ç§„É≥„ÇíÁîüÊàê„Åó„Åæ„Åó„Åü")
        return {"outline": response.content}

    def _generate_article(self, state: ResearchState) -> ResearchState:
        """Generate final article."""
        print("\nüìù ÊúÄÁµÇË®ò‰∫ã„ÇíÁîüÊàê‰∏≠...")
        results_text = "\n\n".join([f"[{r['source']}] {r['title']}\n{r.get('summary', r.get('content', ''))}\nURL: {r['url']}" for r in state["search_results"]])
        
        verification = state.get("verification_report", "")
        verification_note = f"\n\nÊ§úË®ºÁµêÊûú„ÇíËÄÉÊÖÆ„Åó„Å¶„Åè„Å†„Åï„ÅÑ:\n{verification}" if verification else ""
        
        prompt = f"""‰ª•‰∏ã„ÅÆ„Ç¢„Ç¶„Éà„É©„Ç§„É≥„Å´Âæì„Å£„Å¶„ÄÅÂåÖÊã¨ÁöÑ„Å™„É™„Çµ„Éº„ÉÅË®ò‰∫ã„ÇíÊó•Êú¨Ë™û„ÅßÂü∑Á≠Ü„Åó„Å¶„Åè„Å†„Åï„ÅÑ:

{state['outline']}

‰ª•‰∏ã„ÅÆÊÉÖÂ†±Ê∫ê„Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ:
{results_text}
{verification_note}

ÂêÑ‰∏ªÂºµ„ÅÆÂæå„Å´ÂºïÁî®URL [source] „ÇíÂê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
ÁüõÁõæ„Åô„ÇãÊÉÖÂ†±„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ‰∏°Ë´ñ‰ΩµË®ò„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
ÂÖ®„Å¶Êó•Êú¨Ë™û„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"""

        response = self.llm.invoke([SystemMessage(content="„ÅÇ„Å™„Åü„ÅØResearch Writer„Åß„Åô„ÄÇ"), HumanMessage(content=prompt)])
        print("‚úì Ë®ò‰∫ã„ÇíÁîüÊàê„Åó„Åæ„Åó„Åü")
        return {"article": response.content}

    def _build_graph(self) -> StateGraph:
        """Build the research workflow graph."""
        workflow = StateGraph(ResearchState)
        workflow.add_node("generate_subqueries", self._generate_subqueries)
        workflow.add_node("search_sources", self._search_sources)
        workflow.add_node("evaluate_coverage", self._evaluate_coverage)
        workflow.add_node("deep_dive", self._deep_dive)
        workflow.add_node("verify_information", self._verify_information)
        workflow.add_node("generate_outline", self._generate_outline)
        workflow.add_node("generate_article", self._generate_article)

        workflow.set_entry_point("generate_subqueries")
        workflow.add_edge("generate_subqueries", "search_sources")
        workflow.add_edge("search_sources", "evaluate_coverage")
        workflow.add_conditional_edges("evaluate_coverage", self._should_continue_search)
        workflow.add_conditional_edges("deep_dive", self._should_continue_deep_dive)
        workflow.add_edge("verify_information", "generate_outline")
        workflow.add_edge("generate_outline", "generate_article")
        workflow.add_edge("generate_article", END)

        return workflow.compile()

    def research(self, query: str) -> dict:
        """Run research workflow."""
        initial_state: ResearchState = {
            "query": query,
            "subqueries": [],
            "outline": "",
            "search_results": [],
            "article": "",
            "messages": [],
            "iteration": 0,
            "needs_more_search": False,
            "gaps": [],
            "verification_report": "",
            "depth": 0,
            "explored_urls": set(),
        }
        result = self.graph.invoke(initial_state)
        return result
