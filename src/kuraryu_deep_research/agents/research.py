"""Deep Research Agent implementation."""

from langchain_aws import ChatBedrock
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import END, StateGraph

from ..config import Settings
from ..tools import SearchTools
from .state import ResearchState

MAX_ITERATIONS = 3


class DeepResearchAgent:
    """Deep Research Agent using LangGraph."""

    def __init__(self, settings: Settings) -> None:
        """Initialize agent."""
        self.settings = settings
        self.llm = ChatBedrock(
            model_id=settings.model_id,
            region_name=settings.aws_region,
            model_kwargs={"temperature": settings.temperature, "max_tokens": settings.max_tokens},
        )
        self.search_tools = SearchTools()
        self.graph = self._build_graph()

    def _generate_subqueries(self, state: ResearchState) -> ResearchState:
        """Generate subqueries from main query or gaps."""
        iteration = state.get("iteration", 0)
        
        if iteration == 0:
            print("\nðŸ¤” ã‚¹ãƒ†ãƒƒãƒ— 1: ã‚µãƒ–ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆä¸­...")
            target = state["query"]
        else:
            print(f"\nðŸ”„ åå¾© {iteration + 1}: ä¸è¶³æƒ…å ±ã‚’è£œå®Œã™ã‚‹ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆä¸­...")
            target = "ä¸è¶³ã—ã¦ã„ã‚‹è¦³ç‚¹:\n" + "\n".join(state.get("gaps", []))

        prompt = f"""Research Query: "{state['query']}"

{target}

ã“ã®è³ªå•ã«åŒ…æ‹¬çš„ã«ç­”ãˆã‚‹ãŸã‚ã®ã€3-5å€‹ã®å…·ä½“çš„ãªSub Queryã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
Sub Queryã®ã¿ã‚’1è¡Œãšã¤è¿”ã—ã¦ãã ã•ã„ã€‚"""

        response = self.llm.invoke([SystemMessage(content="ã‚ãªãŸã¯Research Assistantã§ã™ã€‚"), HumanMessage(content=prompt)])
        subqueries = [q.strip() for q in response.content.split("\n") if q.strip() and not q.startswith("#")]
        print(f"âœ“ {len(subqueries)}å€‹ã®ã‚µãƒ–ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        
        existing = state.get("subqueries", [])
        return {"subqueries": existing + subqueries, "iteration": iteration + 1}

    def _search_sources(self, state: ResearchState) -> ResearchState:
        """Search multiple sources for information."""
        iteration = state.get("iteration", 1)
        print(f"\nðŸ” æ¤œç´¢ä¸­ (åå¾© {iteration}/{MAX_ITERATIONS})...")
        
        results = list(state.get("search_results", []))
        new_queries = state["subqueries"][-5:]  # æœ€æ–°ã®ã‚µãƒ–ã‚¯ã‚¨ãƒªã®ã¿æ¤œç´¢
        
        for i, subquery in enumerate(new_queries, 1):
            print(f"  [{i}/{len(new_queries)}] {subquery}")
            arxiv_results = self.search_tools.search_arxiv(subquery, max_results=3)
            web_results = self.search_tools.search_web(subquery, max_results=3)
            kaggle_comps = self.search_tools.search_kaggle_competitions(subquery)
            kaggle_datasets = self.search_tools.search_kaggle_datasets(subquery)

            results.extend([{"query": subquery, "source": "arxiv", **r} for r in arxiv_results])
            results.extend([{"query": subquery, "source": "web", **r} for r in web_results])
            results.extend([{"query": subquery, "source": "kaggle-competition", **r} for r in kaggle_comps])
            results.extend([{"query": subquery, "source": "kaggle-dataset", **r} for r in kaggle_datasets])

        print(f"âœ“ åˆè¨ˆ {len(results)}å€‹ã®ã‚½ãƒ¼ã‚¹ã‚’åŽé›†")
        return {"search_results": results}

    def _evaluate_coverage(self, state: ResearchState) -> ResearchState:
        """Evaluate if collected information is sufficient."""
        print("\nðŸ“Š æƒ…å ±ã®ç¶²ç¾…æ€§ã‚’è©•ä¾¡ä¸­...")
        
        results_summary = "\n".join([f"- [{r['source']}] {r['title']}" for r in state["search_results"][:30]])
        
        prompt = f"""ã‚¯ã‚¨ãƒª: "{state['query']}"

åŽé›†ã—ãŸæƒ…å ±æº:
{results_summary}

ã“ã®æƒ…å ±ã§å…ƒã®ã‚¯ã‚¨ãƒªã«ååˆ†ç­”ãˆã‚‰ã‚Œã¾ã™ã‹ï¼Ÿ
- ååˆ†ãªå ´åˆ: ã€ŒSUFFICIENTã€ã¨ã ã‘å›žç­”
- ä¸è¶³ãŒã‚ã‚‹å ´åˆ: ä¸è¶³ã—ã¦ã„ã‚‹è¦³ç‚¹ã‚’ç®‡æ¡æ›¸ãã§åˆ—æŒ™ï¼ˆæœ€å¤§3ã¤ï¼‰"""

        response = self.llm.invoke([SystemMessage(content="ã‚ãªãŸã¯Researchè©•ä¾¡è€…ã§ã™ã€‚"), HumanMessage(content=prompt)])
        
        if "SUFFICIENT" in response.content:
            print("âœ“ æƒ…å ±ã¯ååˆ†ã§ã™")
            return {"needs_more_search": False, "gaps": []}
        
        gaps = [line.strip().lstrip("-â€¢").strip() for line in response.content.split("\n") if line.strip() and not line.startswith("ä¸è¶³")]
        gaps = [g for g in gaps if g][:3]
        print(f"âš  ä¸è¶³ã—ã¦ã„ã‚‹è¦³ç‚¹: {len(gaps)}å€‹")
        for gap in gaps:
            print(f"  - {gap}")
        return {"needs_more_search": True, "gaps": gaps}

    def _should_continue_search(self, state: ResearchState) -> str:
        """Decide whether to continue searching or proceed to outline."""
        if state.get("needs_more_search") and state.get("iteration", 0) < MAX_ITERATIONS:
            return "generate_subqueries"
        return "generate_outline"

    def _generate_outline(self, state: ResearchState) -> ResearchState:
        """Generate article outline from search results."""
        print("\nðŸ“‹ è¨˜äº‹ã®ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã‚’ç”Ÿæˆä¸­...")
        results_text = "\n\n".join([f"- {r['title']}: {r.get('summary', r.get('content', ''))[:200]}" for r in state["search_results"]])
        prompt = f"""ã‚¯ã‚¨ãƒª: "{state['query']}"

æ¤œç´¢çµæžœ:
{results_text}

ã“ã‚Œã‚‰ã®æƒ…å ±ã«åŸºã¥ã„ã¦ã€è©³ç´°ãªè¨˜äº‹ã®ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚
æ—¥æœ¬èªžã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

        response = self.llm.invoke([SystemMessage(content="ã‚ãªãŸã¯Research Writerã§ã™ã€‚"), HumanMessage(content=prompt)])
        print("âœ“ ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        return {"outline": response.content}

    def _generate_article(self, state: ResearchState) -> ResearchState:
        """Generate final article."""
        print("\nðŸ“ æœ€çµ‚è¨˜äº‹ã‚’ç”Ÿæˆä¸­...")
        results_text = "\n\n".join([f"[{r['source']}] {r['title']}\n{r.get('summary', r.get('content', ''))}\nURL: {r['url']}" for r in state["search_results"]])
        prompt = f"""ä»¥ä¸‹ã®ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã«å¾“ã£ã¦ã€åŒ…æ‹¬çš„ãªãƒªã‚µãƒ¼ãƒè¨˜äº‹ã‚’æ—¥æœ¬èªžã§åŸ·ç­†ã—ã¦ãã ã•ã„:

{state['outline']}

ä»¥ä¸‹ã®æƒ…å ±æºã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„:
{results_text}

å„ä¸»å¼µã®å¾Œã«å¼•ç”¨URL [source] ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
å…¨ã¦æ—¥æœ¬èªžã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

        response = self.llm.invoke([SystemMessage(content="ã‚ãªãŸã¯Research Writerã§ã™ã€‚"), HumanMessage(content=prompt)])
        print("âœ“ è¨˜äº‹ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        return {"article": response.content}

    def _build_graph(self) -> StateGraph:
        """Build the research workflow graph."""
        workflow = StateGraph(ResearchState)
        workflow.add_node("generate_subqueries", self._generate_subqueries)
        workflow.add_node("search_sources", self._search_sources)
        workflow.add_node("evaluate_coverage", self._evaluate_coverage)
        workflow.add_node("generate_outline", self._generate_outline)
        workflow.add_node("generate_article", self._generate_article)

        workflow.set_entry_point("generate_subqueries")
        workflow.add_edge("generate_subqueries", "search_sources")
        workflow.add_edge("search_sources", "evaluate_coverage")
        workflow.add_conditional_edges("evaluate_coverage", self._should_continue_search)
        workflow.add_edge("generate_outline", "generate_article")
        workflow.add_edge("generate_article", END)

        return workflow.compile()

    def research(self, query: str) -> dict:
        """Run research workflow."""
        initial_state: ResearchState = {
            "query": query,
            "subqueries": [],
            "outline": "",
            "search_results": [],
            "article": "",
            "messages": [],
            "iteration": 0,
            "needs_more_search": False,
            "gaps": [],
        }
        result = self.graph.invoke(initial_state)
        return result
